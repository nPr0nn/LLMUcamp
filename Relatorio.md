# Relat√≥rio de Implementa√ß√£o e Teste do Projeto LLMUcamp Chatbot  
  
Este relat√≥rio descreve o processo de desenvolvimento do projeto LLMUcamp Chatbot, incluindo o aprendizado sobre RAG (Retrieval-Augmented Generation), o uso e a explora√ß√£o de bibliotecas como LangChain, BeautifulSoup e Streamlit, e a integra√ß√£o delas com um modelo Llama por meio do Groq. Combinando essas tecnologias open source, foi criado um assistente conversacional para informar sobre o vestibular da Unicamp 2025, com base na Resolu√ß√£o GR-029/2024, de 10/07/2024 (https://www.pg.unicamp.br/norma/31879/0). Ao final do processo, foi criado um conjunto de testes para avaliar o chatbot.

As principais ferramentas utilizadas no projeto foram:

- LLM: Llama3-70B-8192
- Framework para NLP: LangChain
- VectorStore: ChromaDB
- Interface Web: Streamlit
  
## Pesquisa e An√°lise Explorat√≥ria

Primeiramente, busquei me familiarizar com o conceito de Retrieval-Augmented Generation (RAG) e sua aplica√ß√£o em tarefas de processamento de linguagem natural (NLP), para isso foi usado uma s√©rie de fontes onlines e ap√≥s ter uma ideia inicial fiz uma v√°rias perguntas ao ChaGPT e busquei checar as respostas por mim mesmo, assim pude validar de forma ativa e entender melhor sobre o assunto. Por fim tentei solidificar ao escrever rascunhos desse relat√≥rio explicando o RAG: 

### RAG (Retrieval-Augmented Generation)

RAG √© uma t√©cnica de NLP que combina recupera√ß√£o de informa√ß√µes (retrieval) com gera√ß√£o de texto (generation). Esse m√©todo tem ganhado destaque por melhorar a qualidade e a precis√£o das respostas geradas por modelos de linguagem, especialmente aqueles baseados em redes neurais profundas. O que de fato condiz com a aplica√ß√£o dessa t√©cnica em ChatBots baseados em modelos Transformers como √© caso desse projeto.

#### Recupera√ß√£o de Informa√ß√µes (Retrieval)

Na primeira etapa, o modelo de RAG utiliza um componente de recupera√ß√£o de informa√ß√µes para buscar dados relevantes em uma base de conhecimento pr√©-existente. Essa base de conhecimento pode ser composta por documentos, artigos, textos ou qualquer outro tipo de informa√ß√£o textual. O objetivo √© encontrar fragmentos de texto que contenham informa√ß√µes relevantes para a pergunta ou contexto fornecido pelo usu√°rio. Para realizar essa recupera√ß√£o, RAG geralmente emprega modelos de embeddings, como o BERT ou outros modelos de Transformer, que s√£o capazes de representar textos em um espa√ßo vetorial. Quando uma consulta √© feita, o modelo transforma tanto a consulta quanto os documentos em vetores e calcula a similaridade entre esses vetores para encontrar os textos mais relevantes.

#### Gera√ß√£o de Texto (Generation)

Uma vez recuperadas as informa√ß√µes relevantes, a segunda etapa envolve a gera√ß√£o de texto. Aqui, o modelo de RAG utiliza um componente de gera√ß√£o, como o GPT (Generative Pre-trained Transformer), para produzir uma resposta coerente e informativa. O diferencial √© que, ao inv√©s de gerar a resposta unicamente com base nos dados aprendidos durante o treinamento (como ocorre em modelos tradicionais de gera√ß√£o de texto), o RAG integra as informa√ß√µes recuperadas na primeira etapa, proporcionando respostas mais precisas e contextualizadas.

#### Integra√ß√£o das Etapas

A integra√ß√£o dessas duas etapas √© o que torna o RAG t√£o poderoso. Quando uma consulta √© recebida, o modelo de recupera√ß√£o identifica as partes mais relevantes da base de conhecimento. Essas partes s√£o ent√£o utilizadas como contexto adicional pelo modelo de gera√ß√£o, que combina esse contexto com seus pr√≥prios conhecimentos para produzir uma resposta final. Isso permite que o modelo acesse uma vasta quantidade de informa√ß√µes durante a gera√ß√£o de respostas, superando limita√ß√µes comuns de modelos que dependem apenas do conhecimento aprendido durante o treinamento.

### Leitura da Resolu√ß√£o 

A seguir, foi realizada a leitura da Resolu√ß√£o GR-029/2024 para se familiarizar com o conte√∫do e facilitar a elabora√ß√£o de perguntas baseadas nessa base de dados. O documento √© bem extenso, em formato de PDF teria por volta de 99 p√°ginas segundo a ferramente de impress√£o, ele, no geral, √©  estruturado em artigos e anexos. Durante essa leitura utilizei do ChatGPT para resumir alguns dos paragrafos para meu melhor entendimento. Embora a maior parte do conte√∫do seja em texto, algumas informa√ß√µes est√£o apresentadas em formato de tabela. Com isso em mente, optei por utilizar ferramentas da LangChain, juntamente com pacotes de web scraping, para extrair as informa√ß√µes necess√°rias para o nosso ChatBot. Essa etapa foi crucial para garantir uma compreens√£o detalhada do material e permitir a cria√ß√£o de perguntas relevantes e bem fundamentadas.

#### Fontes usadas:
- https://aws.amazon.com/pt/what-is/retrieval-augmented-generation/
- https://cloud.google.com/use-cases/retrieval-augmented-generation
- https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/
- https://triggo.ai/blog/o-que-e-retrieval-augmented-generation/
- https://research.ibm.com/blog/retrieval-augmented-generation-RAG
- https://www.pg.unicamp.br/norma/31879/0 


## Implementa√ß√£o e Metodologia

Como apresentado na documenta√ß√£o o c√≥digo desse projeto inteiro est√° contido em apenas 4 arquivos: `fabfile.py`, `source/utils.py`, `source/LLMUcamp.py`, `streamlit_app.py`. Primeiro ser√° descrito o funcionamento do projeto de forma geral e em seguida ser√° detalhado o que cada m√≥dulo faz

### Funcionamento

O sistema √© estruturado em v√°rios m√≥dulos que colaboram para criar uma aplica√ß√£o de chatbot eficiente, utilizando a arquitetura RAG. O **m√≥dulo de gerenciamento de tarefas** coordena a configura√ß√£o e execu√ß√£o do sistema, facilitando a prepara√ß√£o do ambiente, execu√ß√£o de testes e lan√ßamento da aplica√ß√£o. Ele garante que as vari√°veis de ambiente estejam corretamente configuradas e que a vectorstore seja criada e gerenciada.

O **m√≥dulo utilit√°rio** fornece fun√ß√µes auxiliares para carregar e processar dados. Ele √© respons√°vel por criar e carregar a vectorstore a partir de documentos e textos, que s√£o essenciais para o sistema de recupera√ß√£o de informa√ß√µes. Este m√≥dulo integra ferramentas de carregamento de documentos, divis√£o de texto e cria√ß√£o de embeddings, preparando os dados para consulta e recupera√ß√£o.

A **classe de chatbot** configura o modelo de linguagem e o sistema de recupera√ß√£o. Utiliza a vectorstore criada pelo m√≥dulo de utilit√°rios para recuperar informa√ß√µes relevantes e gerar respostas baseadas no contexto da pergunta. Ela configura um modelo de linguagem com um prompt espec√≠fico e implementa um mecanismo de recupera√ß√£o que combina dados do contexto com o modelo de linguagem.

Finalmente, o **m√≥dulo de interface de usu√°rio** oferece uma camada interativa para os usu√°rios. Utilizando uma aplica√ß√£o web, ele permite que os usu√°rios fa√ßam perguntas e recebam respostas do chatbot. Este m√≥dulo se comunica com a classe de chatbot para obter e exibir as respostas em uma interface amig√°vel. No contexto de um sistema RAG, a integra√ß√£o desses m√≥dulos permite a recupera√ß√£o e gera√ß√£o de respostas precisas, combinando dados armazenados com um modelo de linguagem treinado para oferecer uma experi√™ncia de intera√ß√£o enriquecida e contextualizada.

### Fabfile

O arquivo `fabfile.py` utiliza a biblioteca **Fabric** para automa√ß√£o de tarefas, facilitando a execu√ß√£o de fun√ß√µes a partir da linha de comando e tornando o projeto mais facilmente escal√°vel. Ele √© respons√°vel por gerenciar a configura√ß√£o e execu√ß√£o do projeto. Entre suas principais fun√ß√µes, o `setup` cria um arquivo **.env** se n√£o existir, solicitando informa√ß√µes ao usu√°rio para configurar a vari√°vel de ambiente necess√°ria. O c√≥digo para criar o arquivo .env √© o seguinte:

```python
if not dotenv.load_dotenv():
    api_key, rag_url = get_env_info_from_user()
    with open(".env", "w") as f:
        f.write("GROQ_API_KEY=" + api_key + "\nRAG_URL=" + rag_url)
```
Al√©m disso, o `fabfile.py` inclui tarefas para executar o chatbot na linha de comando (`runCLI`) e iniciar a aplica√ß√£o web Streamlit (`runWeb`). A tarefa `test` realiza testes automatizados, perguntando ao chatbot e salvando as respostas, permitindo a verifica√ß√£o da funcionalidade do modelo de linguagem. Importante comentar tamb√©m que √© nesse arquivo que define o LLM base para o chatbot (todos por meio do m√≥dulo Groq), experimentei com os 3 abaixo:

- llama3-70b-8192
- llama-3.1-70b-versatile
- mixtral-8x7b-3276

### Utils

O arquivo `source/utils.py` define v√°rias fun√ß√µes auxiliares que suportam a configura√ß√£o e opera√ß√£o do projeto. Utilizando bibliotecas como `BeautifulSoup (bs4)` para webscrapping e an√°lise de HTML e `LangChain` para processamento de linguagem, este arquivo √© crucial para a cria√ß√£o e manipula√ß√£o da vectorstore. A fun√ß√£o `create_vectorstore`, por exemplo, carrega documentos de um URL e cria embeddings para armazenar em uma vectorstore constru√≠da com o m√≥dulo `Chroma`, que √© uma op√ß√£o lightweight, user-friendly e open-source de um sistema de armazenamento de embeddings vetoriais para modelos de linguagem, e por isso a escolhi inv√™s do usual `FAISS`

```python
def create_vectorstore(docs_url, chunk_size, chunk_overlap, vectorstore_folder):  
	...  
	# Load webpage using langchain and bs4  
	soup_strainer = bs4.SoupStrainer(class_=("card-body"))  
	web_loader = WebBaseLoader(web_paths=(docs_url,), bs_kwargs={"parse_only": soup_strainer})  
	docs = web_loader.load()  
	  
	# Split text into smaller chunks  
	text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True)  
	docs = text_splitter.split_documents(docs)  
	  
	# Create embeddings using langchain and chroma  
	vectorstore = Chroma.from_documents(documents=docs, embedding=HuggingFaceEmbeddings(), persist_directory=vectorstore_folder)  
```
Como pode ser observado os embeddings foram criados com o auxilio do m√≥dulo `HuggingFaceEmbeddings`. Al√©m disso, o arquivo cont√©m fun√ß√µes para carregar a vectorstore e obter caminhos para arquivos de texto, essenciais para o funcionamento e testes do chatbot.

### LLMUcamp

O source/LLMUcamp.py define a classe LLMUcamp, que encapsula a l√≥gica do chatbot. Utilizando a biblioteca `langchain_groq`, a classe configura um modelo de linguagem e um sistema de recupera√ß√£o de informa√ß√µes. A parte do c√≥digo  que inicializa o chatbot e configurar o prompt de seu comportamento √© o seguinte:

```python
class LLMUcamp():  
def __init__(self, vectorstore_folder, model, temperature):  
	self.vectorstore_retriever = utils.load_vectorstore(vectorstore_folder)  
	  
	base_llm = ChatGroq(temperature=temperature, model=model)  
	system_prompt = (  
	"Voc√™ √© um assistente virtual respons√°vel por responder d√∫vidas sobre o Vestibular da Unicamp 2025."  
	"Como fonte de informa√ß√£o, voc√™ usar√° a Resolu√ß√£o GR-029/2024, de 10/07/2024."  
	"Voc√™ dever√° considerar o hist√≥rico da conversa, o contexto e a pergunta dada para fornecer uma resposta."  
	"Caso n√£o saiba uma resposta n√£o tente inventar alguma, responda que n√£o tem uma resposta" 
	"\n\n"  
	"{context}"  
	)  
	  
	prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])  
	answer_chain = create_stuff_documents_chain(base_llm, prompt)  
	self.rag_chain = create_retrieval_chain(self.vectorstore_retriever, answer_chain)  
  
def answer(self, question):  
	response = self.rag_chain.invoke({"input": question})  
	return response.get("answer")
```
A classe fornece uma interface para responder perguntas utilizando o modelo e a vectorstore configurados, permitindo a gera√ß√£o de respostas baseadas em um contexto espec√≠fico e informa√ß√µes armazenadas. Isso permite que a intera√ß√£o com o chatbot seja mais modular em quest√£o de c√≥digo, assim o modo CLI e WEB foram mais f√°ceis de serem feitos

Durante a implementa√ß√£o, testei v√°rias chains da biblioteca `LangChain`, como a `ConversationalRetrievalChain` mas acabou que a abordagem mais satisfat√≥ria e flex√≠vel foi a que envolveu a constru√ß√£o expl√≠cita dos prompts e o retrieval de documentos.

### Streamlit-app

O arquivo streamlit_app.py implementa a interface de usu√°rio do chatbot utilizando a biblioteca `Streamlit`. A aplica√ß√£o web exibe um chat onde os usu√°rios podem interagir com o chatbot. A fun√ß√£o launch_app configura a interface e gerencia a exibi√ß√£o das mensagens:

```python
def launch_app(chatbot: LLMUcamp, user_icon, chatbot_icon):  
	st.title("üéì Chatbot Vestibular Unicamp 2025 - LLMUcamp")  	  
	avatar_dict = {"user": user_icon, "assistant": chatbot_icon}  
	  
	# Initialize the chat messages history  
	if "messages" not in st.session_state.keys():  
	st.session_state.messages = [{  
		"role": "assistant",  
		"content": "Ol√°! Estou aqui para ajudar..."
	}]  
	  
	# Prompt for user input and save  
	if prompt := st.chat_input():  
	st.session_state.messages.append({"role": "user", "content": prompt})  
	  
	# Display the existing chat messages  
	for message_data in st.session_state.messages:  
	role = message_data["role"]  
	content = message_data["content"]  
	with st.chat_message(role, avatar=avatar_dict[role]):  
	st.markdown(f"{content}")  
	  
	# If last message is not from assistant, 
	# we need to generate a new answer  
	if st.session_state.messages[-1]["role"] != "assistant":  
	# Call the chatbot function  
	with st.chat_message("assistant", avatar=avatar_dict["assistant"]):  
	with st.spinner("Pensando..."):  
	answer = chatbot.answer(prompt)
	st.write_stream(typed_answer(answer))  
	  
	message = {"role": "assistant", "content": answer}  
	st.session_state.messages.append(message)
```
A interface √© personalizada de forma que:
- O avatares da conversa fossem mais relacionados com o objetivo.
- As mensagens do bot e do usu√°rio fiquem em lados opostos para que fique mais similar a um chat de rede social
- A resposta do chatbot √© gerada e exibida com um efeito de digita√ß√£o para simular uma conversa natural.

#### Fontes usadas:
- https://python.langchain.com/v0.2/docs/tutorials/chatbot/
- https://console.groq.com/docs/quickstart
- https://python.langchain.com/v0.2/docs/integrations/text_embedding/sentence_transformers/
- https://docs.streamlit.io/get-started/tutorials/create-an-app
- https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338
- https://docs.streamlit.io/


## Testes e Avalia√ß√£o

### Testes 

Para avaliar o chatbot foi desenvolvido 2 conjuntos de teste. O primeiro contendo 100 perguntas geradas pelo ChatGPT acerca do vestibular da Unicamp. E o segundo contendo 10 perguntas criadas por um ser humano. Ambos os conjuntos est√£o em arquivos .txt na pasta `tests_data/questions`. Ao rodar o comando abaixo cada uma das perguntas ser√° passada ao chatbot e as respostas ser√£o salvas na pasta `tests_data/answers`

```bash
fab test
```

Caso deseje adicionar suas pr√≥prias perguntas basta criar um arquivo .txt com elas dentro da pasta `tests_data/questions` (precisam estar numeradas)

### Avalia√ß√£o

Avaliando manualmente as respostas dos testes nos arquivos .txt foi possivel concluir n√£o s√≥ que o melhor modelo base foi o `llama-3.1-70b-versatile` como todos eles conseguiram  capturar um bom conhecimento relacionado ao Vestibular da Unicamp 2025. As perguntas em que ele se saiu melhor foram as mais abertas e que pedem instru√ß√µes ou perguntam sobre a exist√™ncia de algo, o que indica um bom comportamento como assistente simples. Por√©m, ficou not√°vel algumas dificuldades em perguntas mais objetivas como as datas de inscri√ß√£o do vestibular ou n√∫meros de vagas, acredito que tenha haver com a quest√£o de obter informa√ß√µes sobre as tabelas. Importante comentar que depois que o preview do Groq do modelo `llama-3.1-70b-versatile` atingiu o seu limite passei a utilizar o `llama-3-70b-8192` que n√£o tinha esse limite, assim todas as ferramentas usadas no projeto foram gratuitas.

## Uso do ChatGPT

Ao longo do relat√≥rio foi explicado em maiores detalhes cada parte que utilizei o ChatGPT, mas para fins de organiza√ß√£o aqui est√° uma lista de atividades em que ele foi √∫til:

- Clarificar e responder minhas d√∫vidas sobre RAG
- Resumo de paragrafos da resolu√ß√£o do vestibular da unicamp 2025
- Auxiliar de documenta√ß√£o do Streamlit e do LangChain
- Gera√ß√£o de 100 perguntas para teste do chatbot
- Corre√ß√£o e melhora de textos para a documenta√ß√£o e relat√≥rio
